{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Deep Embeddings\n",
    "\n",
    "Paper Link - https://arxiv.org/abs/1611.05148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import config\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.contrib.distributions\n",
    "xav_init = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Binarizing MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.contrib.learn.datasets.mnist.load_mnist(train_dir=\"mnist_data\")\n",
    "\n",
    "test_data = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.images[mnist.train.images < 0.5] = 0\n",
    "mnist.train.images[mnist.train.images > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    global X\n",
    "    \n",
    "    h_encoders = [\n",
    "        tf.layers.dense(\n",
    "            X,\n",
    "            config.encoder_hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"encoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(config.encoder_hidden_size[1:]):\n",
    "        h_encoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_encoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"encoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    encoder_mean = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        config.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_mean\"\n",
    "    )\n",
    "    encoder_log_var = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        config.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_log_variance\"\n",
    "    )\n",
    "    \n",
    "    return encoder_mean, encoder_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder():\n",
    "    global Z\n",
    "    \n",
    "    h_decoders = [\n",
    "        tf.layers.dense(\n",
    "            Z,\n",
    "            config.decoder_hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"decoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(config.decoder_hidden_size[1:]):\n",
    "        h_decoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_decoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"decoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    out_X = tf.layers.dense(\n",
    "        h_decoders[-1],\n",
    "        config.input_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"decoder_X\"\n",
    "    )\n",
    "    \n",
    "    return out_X, tf.nn.sigmoid(out_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Z using the reparametrization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z():\n",
    "    global epsilon\n",
    "    global encoder_mean, encoder_log_var\n",
    "    \n",
    "    return encoder_mean + tf.exp(encoder_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and Learning the GMM Priors (Pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prior():\n",
    "    prior_means = tf.Variable(\n",
    "        tf.random_normal((config.n_clusters, config.latent_dim), stddev=5.0),\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_means\"\n",
    "    )\n",
    "    prior_vars = tf.Variable(\n",
    "        tf.ones((config.n_clusters, config.latent_dim)),\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_vars\"\n",
    "    )\n",
    "    prior_weights = tf.Variable(\n",
    "        tf.ones((config.n_clusters)) / config.n_clusters,\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_weights\"\n",
    "    )\n",
    "    \n",
    "    return prior_means, prior_vars, prior_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gmm_priors(Z=None, train=True):\n",
    "    global init_gmm_model\n",
    "    \n",
    "    if train == True:\n",
    "        init_gmm_model.fit(Z)\n",
    "        \n",
    "    return init_gmm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Posterior of Cluster Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_c():\n",
    "    global Z\n",
    "    global prior_means, prior_vars, prior_weights\n",
    "    \n",
    "    def fn_cluster(_, k):\n",
    "        q = prior_weights[k] * ds.MultivariateNormalDiag(loc=prior_means[k], scale_diag=prior_vars[k]).prob(Z) + 1e-10\n",
    "        return tf.reshape(q, [config.batch_size])\n",
    "\n",
    "    clusters = tf.Variable(tf.range(config.n_clusters))\n",
    "    probs = tf.scan(fn_cluster, clusters, initializer=tf.ones([config.batch_size]))\n",
    "    probs = tf.transpose(probs)\n",
    "    probs = probs / tf.reshape(tf.reduce_sum(probs, 1), (-1, 1))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss():\n",
    "    global cluster_weights\n",
    "    global X, decoded_exp_X_mean\n",
    "    global encoder_mean, encoder_log_var\n",
    "    global prior_means, prior_vars, prior_weights\n",
    "    \n",
    "    J = 0.0\n",
    "    J += config.regularizer * tf.reduce_sum(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=decoded_exp_X_mean),\n",
    "        axis=1\n",
    "    )\n",
    "    J -= tf.reduce_sum(cluster_weights * tf.log(prior_weights), axis=1)\n",
    "    J += tf.reduce_sum(cluster_weights * tf.log(cluster_weights), axis=1)\n",
    "    J -= 0.5 * tf.reduce_sum(1 + encoder_log_var, axis=1)\n",
    "\n",
    "    def fn_cluster(previous_output, current_input):\n",
    "        k = current_input\n",
    "        l = previous_output + 0.5 * cluster_weights[:, k] * tf.reduce_sum(\n",
    "            tf.log(prior_vars[k]) + (tf.exp(encoder_log_var) + tf.square(encoder_mean - prior_means[k])) / prior_vars[k], axis=1\n",
    "        )\n",
    "        return l\n",
    "\n",
    "    clusters = tf.Variable(tf.range(config.n_clusters))\n",
    "    y = tf.scan(fn_cluster, clusters, initializer=tf.zeros(config.batch_size))\n",
    "    \n",
    "    J += y[-1, :]\n",
    "    \n",
    "    return tf.reduce_mean(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_len = int(len(mnist.train.images) / config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, config.input_dim])\n",
    "epsilon = tf.placeholder(tf.float32, [None, config.latent_dim])\n",
    "\n",
    "prior_means, prior_vars, prior_weights = init_prior()\n",
    "\n",
    "encoder_mean, encoder_log_var = encoder()\n",
    "\n",
    "Z = sample_Z()\n",
    "\n",
    "decoded_exp_X_mean, decoded_X_mean = decoder()\n",
    "\n",
    "cluster_weights = q_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae_loss()\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=config.adam_learning_rate,\n",
    "    global_step=0,\n",
    "    decay_steps=epoch_len * config.adam_decay_steps,\n",
    "    decay_rate=config.adam_decay_rate\n",
    ")\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=config.adam_epsilon).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Pretraining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_loss = tf.reduce_mean(\n",
    "    tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=decoded_exp_X_mean), axis=1) \\\n",
    "    + 0.5 * tf.reduce_sum(tf.exp(encoder_log_var) + encoder_mean ** 2 - 1. - encoder_log_var, axis=1)\n",
    ")\n",
    "pretrain_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(pretrain_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regeneration_plot(epoch):\n",
    "    if not os.path.exists(\"plots/regenerated\"):\n",
    "        os.makedirs(\"plots/regenerated\")\n",
    "    \n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    decoded_image = sess.run(\n",
    "        [decoded_X_mean],\n",
    "        feed_dict={\n",
    "            X: test_data[:100],\n",
    "            epsilon: np.random.randn(100, config.latent_dim)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = grid.GridSpec(1, 2) \n",
    "\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "\n",
    "    decoded_image = np.array(decoded_image).reshape((100, 784))\n",
    "    figure = np.zeros((280, 280))\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        for j in range(0, 10):\n",
    "            figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "\n",
    "    ax1.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    decoded_image = np.array(test_data[:100])\n",
    "    figure = np.zeros((280, 280))\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        for j in range(0, 10):\n",
    "            figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "\n",
    "    ax2.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/regenerated/\" + str(epoch) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_plot(epoch):\n",
    "    if not os.path.exists(\"plots/sampled\"):\n",
    "        os.makedirs(\"plots/sampled\")\n",
    "    \n",
    "    mus, sigmas = sess.run([prior_means, prior_vars], feed_dict={})\n",
    "    \n",
    "    sigmas = np.sqrt(sigmas)\n",
    "    \n",
    "    figure = np.zeros((280, 280))\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    for k in range(0, 10):\n",
    "        for i in range(0, 10):\n",
    "            eps = np.random.randn(1, config.latent_dim)\n",
    "            sample = eps * sigmas[k] + mus[k]\n",
    "            \n",
    "            decoded_image = sess.run(\n",
    "                decoded_X_mean,\n",
    "                feed_dict={\n",
    "                    Z: sample\n",
    "                }\n",
    "            ).reshape((28, 28)) * 255\n",
    "\n",
    "            figure[k * 28 : (k + 1) * 28, i * 28 : (i + 1) * 28] = decoded_image\n",
    " \n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/sampled/\" + str(epoch) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining for VAE parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(config.pretrain_vae_n_epochs):\n",
    "    J = 0.0\n",
    "    for i in tqdm(range(epoch_len)):\n",
    "        X_batch = mnist.train.next_batch(config.batch_size)[0]\n",
    "        out = sess.run(\n",
    "            [pretrain_loss, pretrain_step],\n",
    "            feed_dict={\n",
    "                X: X_batch,\n",
    "                epsilon: np.random.randn(config.batch_size, config.latent_dim)\n",
    "            }\n",
    "        )\n",
    "        J += out[0] / epoch_len\n",
    "        \n",
    "    regeneration_plot(epoch)\n",
    "    \n",
    "    print(\"Pretrain Epoch %d: %.3f\" % (epoch + 1, J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining for GMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = sess.run(Z, feed_dict={\n",
    "    X: mnist.train.next_batch(config.pretrain_gmm_train_size)[0],\n",
    "    epsilon: np.random.randn(config.pretrain_gmm_train_size, config.latent_dim)\n",
    "})\n",
    "\n",
    "init_gmm_model = mixture.GaussianMixture(\n",
    "    n_components=config.n_clusters,\n",
    "    covariance_type=\"diag\",\n",
    "    max_iter=config.pretrain_gmm_n_iters,\n",
    "    n_init=config.pretrain_gmm_n_inits,\n",
    "    weights_init=np.ones(config.n_clusters) / config.n_clusters,\n",
    ")\n",
    "\n",
    "init_gmm_means = tf.assign(prior_means, init_gmm_priors(Z=lv).means_)\n",
    "init_gmm_vars = tf.assign(prior_vars, init_gmm_priors(train=False).covariances_)\n",
    "init_gmm_weights = tf.assign(prior_weights, init_gmm_priors(train=False).weights_)\n",
    "\n",
    "_ = sess.run([init_gmm_means, init_gmm_vars, init_gmm_weights], feed_dict={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the VaDE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(config.n_epochs):\n",
    "    J = 0.0\n",
    "    for i in tqdm(range(epoch_len)):\n",
    "        X_batch = mnist.train.next_batch(config.batch_size)[0]\n",
    "        out = sess.run(\n",
    "            [loss, train_step],\n",
    "            feed_dict={\n",
    "                X: X_batch,\n",
    "                epsilon: np.random.randn(config.batch_size, config.latent_dim)\n",
    "            }\n",
    "        )\n",
    "        J += out[0] / epoch_len\n",
    "    \n",
    "    print(\"Epoch %d: %.3f\" % (epoch + 1, J))\n",
    "    \n",
    "    sample_plot(epoch)\n",
    "    regeneration_plot(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
