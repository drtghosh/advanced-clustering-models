{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import config\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.contrib.learn.datasets.mnist.load_mnist(train_dir=\"mnist_data\")\n",
    "\n",
    "test_data = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.next_batch(100)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mnist.train.next_batch(10)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.images[mnist.train.images < 0.5] = 0\n",
    "mnist.train.images[mnist.train.images > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.contrib.distributions\n",
    "xav_init = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(X, hidden_size):\n",
    "    h_encoders = [\n",
    "        tf.layers.dense(\n",
    "            X,\n",
    "            hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"encoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(hidden_size[1:]):\n",
    "        h_encoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_encoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"encoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    encoder_mean = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        config.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_mean\"\n",
    "    )\n",
    "    encoder_log_var = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        config.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_log_variance\"\n",
    "    )\n",
    "    \n",
    "    return encoder_mean, encoder_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(Z, hidden_size):\n",
    "    h_decoders = [\n",
    "        tf.layers.dense(\n",
    "            Z,\n",
    "            hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"decoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(hidden_size[1:]):\n",
    "        h_decoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_decoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"decoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    out_X = tf.layers.dense(\n",
    "        h_decoders[-1],\n",
    "        config.input_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"decoder_X\"\n",
    "    )\n",
    "    \n",
    "    return out_X, tf.nn.sigmoid(out_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(encoder_mean, encoder_log_var, epsilon):\n",
    "    return encoder_mean + tf.exp(encoder_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prior():\n",
    "    prior_means = tf.Variable(\n",
    "        tf.random_normal((config.n_clusters, config.latent_dim), stddev=5.0),\n",
    "        dtype=tf.float32,\n",
    "#         trainable=False,\n",
    "        name=\"prior_means\"\n",
    "    )\n",
    "    prior_vars = tf.Variable(\n",
    "        tf.ones((config.n_clusters, config.latent_dim)),\n",
    "        dtype=tf.float32,\n",
    "#         trainable=False,\n",
    "        name=\"prior_vars\"\n",
    "    )\n",
    "    prior_weights = tf.Variable(\n",
    "        tf.ones((config.n_clusters)) / config.n_clusters,\n",
    "        dtype=tf.float32,\n",
    "        trainable=False,\n",
    "        name=\"prior_weights\"\n",
    "    )\n",
    "    \n",
    "    return prior_means, prior_vars, prior_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_c(Z):\n",
    "    def fn_cluster(_, k):\n",
    "        q = prior_weights[k] * ds.MultivariateNormalDiag(loc=prior_means[k], scale_diag=prior_vars[k]).prob(Z) + 1e-10\n",
    "        return tf.reshape(q, [config.batch_size])\n",
    "\n",
    "    clusters = tf.Variable(tf.range(config.n_clusters))\n",
    "    probs = tf.scan(fn_cluster, clusters, initializer=tf.ones([config.batch_size]))\n",
    "    probs = tf.transpose(probs)\n",
    "    probs = probs / tf.reshape(tf.reduce_sum(probs, 1), (-1, 1))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(cluster_weights, decoded_X_mean, encoder_mean, encoder_log_var):\n",
    "    J = 0.0\n",
    "    J += config.regularizer * tf.reduce_sum(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=decoded_exp_X_mean),\n",
    "        axis=1\n",
    "    )\n",
    "    J -= tf.reduce_sum(cluster_weights * tf.log(prior_weights), axis=1)\n",
    "    J += tf.reduce_sum(cluster_weights * tf.log(cluster_weights), axis=1)\n",
    "    J -= 0.5 * tf.reduce_sum(1 + encoder_log_var, axis=1)\n",
    "\n",
    "    def loop_cluster(previous_output, current_input):\n",
    "        k = current_input\n",
    "        l = previous_output + 0.5 * cluster_weights[:, k] * tf.reduce_sum(\n",
    "            tf.log(prior_vars[k]) + (tf.exp(encoder_log_var) + tf.square(encoder_mean - prior_means[k])) / prior_vars[k], axis=1\n",
    "        )\n",
    "        return l\n",
    "\n",
    "    clusters = tf.Variable(tf.range(config.n_clusters))\n",
    "    y = tf.scan(loop_cluster, clusters, initializer=tf.zeros(config.batch_size))\n",
    "    \n",
    "    J += y[-1, :]\n",
    "    \n",
    "    return tf.reduce_mean(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prior_weights_f(prior_weights, cluster_weights):\n",
    "    return tf.reduce_sum(cluster_weights, axis=0) / float(config.batch_size)\n",
    "\n",
    "def update_prior_means_f(prior_means, encoder_mean, cluster_weights):\n",
    "    def loop_cluster(_, k):\n",
    "        t = tf.matmul(tf.reshape(cluster_weights[:, k], [1, config.batch_size]), encoder_mean)\n",
    "        t = tf.reshape(t, [config.latent_dim]) / tf.reduce_sum(cluster_weights[:, k], axis=0)\n",
    "        return t\n",
    "\n",
    "    clusters = tf.Variable(tf.range(config.n_clusters))\n",
    "    return tf.scan(loop_cluster, clusters, initializer=tf.ones([config.latent_dim]))\n",
    "\n",
    "def update_prior_vars_f(prior_weights, encoder_mean, encoder_log_var, cluster_weights):\n",
    "    global prior_means\n",
    "    \n",
    "    s = tf.matmul(\n",
    "        tf.reshape(cluster_weights[:, 0], [1, config.batch_size]),\n",
    "        (tf.exp(encoder_log_var) + tf.square(encoder_mean - prior_means[0]))\n",
    "    ) / tf.reduce_sum(cluster_weights[:, 0], axis=0)\n",
    "    \n",
    "    for i in range(1, config.n_clusters):\n",
    "        t = tf.matmul(\n",
    "            tf.reshape(cluster_weights[:, i], [1, config.batch_size]),\n",
    "            tf.exp(encoder_log_var) + tf.square(encoder_mean - prior_means[i])\n",
    "        ) / tf.reduce_sum(cluster_weights[:, i], axis=0)\n",
    "        s = tf.concat([s, t], 0)\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_len = int(len(mnist.train.images) / config.batch_size)\n",
    "decay_steps = epoch_len * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, config.input_dim])\n",
    "epsilon = tf.placeholder(tf.float32, [None, config.latent_dim])\n",
    "\n",
    "prior_means, prior_vars, prior_weights = init_prior()\n",
    "\n",
    "encoder_mean, encoder_log_var = encoder(X, config.encoder_hidden_size)\n",
    "\n",
    "Z = sample_Z(encoder_mean, encoder_log_var, epsilon)\n",
    "\n",
    "decoded_exp_X_mean, decoded_X_mean = decoder(Z, config.decoder_hidden_size)\n",
    "\n",
    "cluster_weights = q_c(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae_loss(cluster_weights, decoded_X_mean, encoder_mean, encoder_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_prior_weights = prior_weights.assign(update_prior_weights_f(prior_weights, cluster_weights))\n",
    "# update_prior_means = prior_means.assign(update_prior_means_f(prior_means, encoder_mean, cluster_weights))\n",
    "# update_prior_vars = prior_vars.assign(update_prior_vars_f(prior_vars, encoder_mean, encoder_log_var, cluster_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = tf.train.exponential_decay(.002, 0, decay_steps, 0.9, staircase=True)\n",
    "train_step = tf.train.AdamOptimizer(config.adam_nn_learning_rate, epsilon=config.adam_nn_epsilon).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regeneration_plot(epoch):\n",
    "    if not os.path.exists(\"plots/regenerated\"):\n",
    "        os.makedirs(\"plots/regenerated\")\n",
    "    \n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    decoded_image = sess.run(\n",
    "        [decoded_X_mean],\n",
    "        feed_dict={\n",
    "            X: test_data[:100],\n",
    "            epsilon: np.random.randn(100, config.latent_dim)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = grid.GridSpec(1, 2) \n",
    "\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "\n",
    "    decoded_image = np.array(decoded_image).reshape((100, 784))\n",
    "    figure = np.zeros((280, 280))\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        for j in range(0, 10):\n",
    "            figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "\n",
    "    ax1.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    decoded_image = np.array(test_data[:100])\n",
    "    figure = np.zeros((280, 280))\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        for j in range(0, 10):\n",
    "            figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "\n",
    "    ax2.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/regenerated/\" + str(epoch) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_plot(epoch):\n",
    "    if not os.path.exists(\"plots/sampled\"):\n",
    "        os.makedirs(\"plots/sampled\")\n",
    "    \n",
    "    mus, sigmas = sess.run([prior_means, prior_vars], feed_dict={})\n",
    "    \n",
    "    figure = np.zeros((280, 280))\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    for k in range(0, 10):\n",
    "        for i in range(0, 10):\n",
    "            eps = np.random.randn(1, config.latent_dim)\n",
    "            decoded_image = sess.run(\n",
    "                decoded_X_mean,\n",
    "                feed_dict={\n",
    "                    Z: eps * sigmas[k] + mus[k]\n",
    "                }\n",
    "            ).reshape((28, 28)) * 255\n",
    "\n",
    "            figure[k * 28 : (k + 1) * 28, i * 28 : (i + 1) * 28] = decoded_image\n",
    " \n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/sampled/\" + str(epoch) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(config.n_epochs):\n",
    "    J = 0.0\n",
    "    for i in tqdm(range(epoch_len)):\n",
    "        X_batch = mnist.train.next_batch(config.batch_size)[0]\n",
    "        out = sess.run(\n",
    "            [loss, train_step],\n",
    "            feed_dict={\n",
    "                X: X_batch,\n",
    "                epsilon: np.random.randn(config.batch_size, config.latent_dim)\n",
    "            }\n",
    "        )\n",
    "        J += out[0] / epoch_len\n",
    "    \n",
    "    print(J)\n",
    "    \n",
    "    sample_plot(epoch)\n",
    "    regeneration_plot(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
